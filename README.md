# Webscraping and Analysis of CO2 Emissions Data
## Project Overview:

Welcome to my second project, where I leveraged the knowledge gained from a bootcamp to delve into the realm of data visualization. In this project, I embarked on web scraping the [Worldometer CO2 Emissions data](https://www.worldometers.info/co2-emissions/co2-emissions-by-country/) to shed light on the top emitting countries across the globe.

### Key Steps:

1. **Data Collection:** Utilized web scraping techniques to extract relevant CO2 emissions data from the Worldometer website.
   
2. **Data Storage:** Established a SQL table hosted on Clever Cloud to organize and store the acquired data efficiently.
   
3. **Analysis and Visualization:** Leveraged Jupyter Notebooks alongside powerful visualization libraries like Seaborn and Matplotlib to craft insightful visual representations of the emissions data. Through SQL queries, I seamlessly integrated the stored data into my analysis pipeline.

### Project Goals:

- **Insight Generation:** The primary objective was to uncover trends and patterns within the CO2 emissions landscape, highlighting the countries with the highest emissions.

- **Skill Development:** This project served as a platform to hone my skills in web scraping, data manipulation, SQL querying, and data visualization, further solidifying my proficiency in these areas.

## Tools and Technologies

- **Web Scraping**: To gather data from an online source.
- **SQL**: To organize and store the acquired data efficiently.
- **Pandas**: For data manipulation and analysis.

## Project Structure

- `data_analysis_visualization`: Contains the Jupyter Notebook with detailed analysis.

- `Webscrapingcode`: Contains the code used to colect and store the data.


